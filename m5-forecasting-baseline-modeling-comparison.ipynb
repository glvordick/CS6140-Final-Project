{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Contents\n\n* [<font size=4>Abstract</font>](#1)\n* [<font size=4>Objective</font>](#1)\n* [<font size=4>The dataset</font>](#1)\n* [<font size=4>Performance Metric</font>](#1)\n\n* [<font size=4>EDA</font>](#2)\n  \n    \n* [<font size=4>Modeling</font>](#3)\n    * [Train/Val split](#3.1)\n    * [Naive approach](#3.2)\n    * [Moving average](#3.3)\n    * [Exponential smoothing](#3.4)\n    * [ARIMA](#3.5)\n    * [Prophet](#3.6)\n    * [Loss for each model](#3.7)\n\n","metadata":{}},{"cell_type":"markdown","source":"# Abstract\n\nWalmart, Whole Foods, Amazon, and other large department stores maintain millions of products and record millions of transactions every day. In order to optimize the profit, store manager must keep balance between inventory, current demand, and they are relying on accurate sales prediction to make informed decisions. Most of the existing sales predictions depends on statistical inference from trends. previous forecasting methods requires a lot of extra information from customers to product detailed information and requires customer and product attribute analysis. In this project we will try to build simplified model to predict the unit sales based historical sales record. Our data is from Kaggle M5 Competition, we will use combination of statistical a machine learning including Deep Neural Networks to solve this exciting case study.\n","metadata":{}},{"cell_type":"markdown","source":"# Objective\n\nThe objective of the project is to precisely forecast the unit sales for Walmart USA. Incorrect or misleading forecasts on product sales could trigger opportunity and revenue loss for Walmart. For instance, if analyst team failed to estimate the demand for different product for different categories at different stores located in various states during long weekends or thanksgivings, they may lose opportunity to attract thousands of customers and potentially lose millions of dollars of revenue.  We will build a model that will assist the business analyst and manager to improve planning the inventory distribution, inventory storage solutions, promotions and offer decisions.\n","metadata":{}},{"cell_type":"markdown","source":"# The Dataset Information\n\nThe dataset consists of five .csv files.\n\n1. calendar.csv - Contains the dates on which products are sold. The dates are in a yyyy/dd/mm format.\n\n2. sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913].\n\n3. submission.csv - Demonstrates the correct format for submission to the competition.\n\n4. sell_prices.csv - Contains information about the price of the products sold per store and date.\n\n5. sales_train_evaluation.csv - Available one month before the competition deadline. It will include sales for [d_1 - d_1941].\n\n* In this competition, we need to forecast the sales for [d_1942 - d_1969]. These rows form the evaluation set. The rows [d_1914 - d_1941] form the validation set, and the remaining rows form the training set. Now, since we understand the dataset and know what to predict, let us visualize the dataset.\n\n* Dataset was provided by Walmart USA, involves unit sales of various product sold in the USA, organized in the form of grouped time series.\n*\tDataset contains unique 3049 products, classified in 3 product categories (Hobbies, Food, and Household), and 7 product departments, products are sold across 10 stores in 3 different states (CA, TX, WI )\n*\tIn this respect, the bottom-level of the hierarchy, i.e., product-store unit sales can be mapped across either product categories or geographical regions, as follows:\n* Every product has selling history of 1941 days, (test data of h=28 days not included)\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/WB6hm1l.png)","metadata":{}},{"cell_type":"markdown","source":"# Perfromance Metric\n\nThe accuracy of the point forecasts will be evaluated using the R**oot Mean Squared Scaled Error (RMSSE)**, The measure is calculated for each series as follows.\n\n> $RMSSE =\\sqrt{\\frac{1}{h} * \\frac{\\sum_{t=n+1}^{t=n+h} {(Y_t - \\hat{Y_t}})^2}{\\frac{1}{n-1}*(\\sum_{t=2}^{t=n}(y_t-y_{t-1)})^2}}$\n\nwhere Yt  is the actual future value of the examined time series at point t,  the generated forecast, n the length of the training sample (number of historical observations), and h the forecasting horizon. After estimating RMSSE for all the time series of the competition, participant method will be ranked using** Weighted RMSSE**.\n\n> $WRMSSE =\\sum_{i=1}^{i=42840} {w_i * RMSSE}$\n\n\nwhere W_i is the weight of the ith series of the competition?  n is the total number of time series in competition, A lower WRMSSE score is better. *weight of each series will be computed based on the last 28 observations of the training sample of the dataset*, i.e., **the cumulative actual dollar sales that each series displayed in that period (sum of units sold multiplied by their respective price**).\n","metadata":{}},{"cell_type":"markdown","source":"### Import Library","metadata":{}},{"cell_type":"code","source":"\nimport scipy\nimport statsmodels\nfrom scipy import signal\nimport statsmodels.api as sm\nfrom fbprophet import Prophet\nfrom scipy.signal import butter, deconvolve\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom math import log, floor\nfrom sklearn.neighbors import KDTree\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom tqdm.notebook import tqdm as tqdm\n\nimport seaborn as sns\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport pywt\nfrom statsmodels.robust import mad\n\nimport os\nimport gc\nimport time\nimport math\nimport datetime\n\nfrom plotly.offline import plot, iplot, init_notebook_mode,plot_mpl\nfrom plotly.offline import init_notebook_mode, plot_mpl\ninit_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:03:52.895042Z","iopub.execute_input":"2022-05-01T02:03:52.895454Z","iopub.status.idle":"2022-05-01T02:03:58.375193Z","shell.execute_reply.started":"2022-05-01T02:03:52.89536Z","shell.execute_reply":"2022-05-01T02:03:58.374088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the data","metadata":{}},{"cell_type":"code","source":"calendar_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\nsells_price_df = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\nsales_train_validation_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nsales_train_evalution_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\nsubmission_df = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:03:58.377505Z","iopub.execute_input":"2022-05-01T02:03:58.377857Z","iopub.status.idle":"2022-05-01T02:04:17.251213Z","shell.execute_reply.started":"2022-05-01T02:03:58.377813Z","shell.execute_reply":"2022-05-01T02:04:17.250471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CSS color: for plotly graphics\n\n#                 aliceblue, antiquewhite, aqua, aquamarine, azure,\n#                 beige, bisque, black, blanchedalmond, blue,\n#                 blueviolet, brown, burlywood, cadetblue,\n#                 chartreuse, chocolate, coral, cornflowerblue,\n#                 cornsilk, crimson, cyan, darkblue, darkcyan,\n#                 darkgoldenrod, darkgray, darkgrey, darkgreen,\n#                 darkkhaki, darkmagenta, darkolivegreen, darkorange,\n#                 darkorchid, darkred, darksalmon, darkseagreen,\n#                 darkslateblue, darkslategray, darkslategrey,\n#                 darkturquoise, darkviolet, deeppink, deepskyblue,\n#                 dimgray, dimgrey, dodgerblue, firebrick,\n#                 floralwhite, forestgreen, fuchsia, gainsboro,\n#                 ghostwhite, gold, goldenrod, gray, grey, green,\n#                 greenyellow, honeydew, hotpink, indianred, indigo,\n#                 ivory, khaki, lavender, lavenderblush, lawngreen,\n#                 lemonchiffon, lightblue, lightcoral, lightcyan,\n#                 lightgoldenrodyellow, lightgray, lightgrey,\n#                 lightgreen, lightpink, lightsalmon, lightseagreen,\n#                 lightskyblue, lightslategray, lightslategrey,\n#                 lightsteelblue, lightyellow, lime, limegreen,\n#                 linen, magenta, maroon, mediumaquamarine,\n#                 mediumblue, mediumorchid, mediumpurple,\n#                 mediumseagreen, mediumslateblue, mediumspringgreen,\n#                 mediumturquoise, mediumvioletred, midnightblue,\n#                 mintcream, mistyrose, moccasin, navajowhite, navy,\n#                 oldlace, olive, olivedrab, orange, orangered,\n#                 orchid, palegoldenrod, palegreen, paleturquoise,\n#                 palevioletred, papayawhip, peachpuff, peru, pink,\n#                 plum, powderblue, purple, red, rosybrown,\n#                 royalblue, saddlebrown, salmon, sandybrown,\n#                 seagreen, seashell, sienna, silver, skyblue,\n#                 slateblue, slategray, slategrey, snow, springgreen,\n#                 steelblue, tan, teal, thistle, tomato, turquoise,\n#                 violet, wheat, white, whitesmoke, yellow,\n#                 yellowgreen","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:17.253379Z","iopub.execute_input":"2022-05-01T02:04:17.253702Z","iopub.status.idle":"2022-05-01T02:04:17.259794Z","shell.execute_reply.started":"2022-05-01T02:04:17.253656Z","shell.execute_reply":"2022-05-01T02:04:17.258946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"We will explore following models:\n\n1. Naive Approach\n2. Moving Average\n3. Holt Linear\n4. Exponential Smoothing\n5. ARIMA\n6. Facebook's Prophet\n\n","metadata":{}},{"cell_type":"markdown","source":"## Train Test Split\n\n\n* We need to split sales train data into validation and train split, we will use last 50 days data as validation data, and 100 days before that as a train data. \n* We will build model using train data and test it on validaton data.\n\n","metadata":{}},{"cell_type":"code","source":"sales_train_validation_df.tail()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:17.262102Z","iopub.execute_input":"2022-05-01T02:04:17.262877Z","iopub.status.idle":"2022-05-01T02:04:17.305475Z","shell.execute_reply.started":"2022-05-01T02:04:17.262831Z","shell.execute_reply":"2022-05-01T02:04:17.304563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_colums = [ col for col in sales_train_validation_df if 'd_' in col]\ntrain_data = sales_train_validation_df[data_colums[:-30]]\ntrain_data = sales_train_validation_df[data_colums[-100:-30]]\nvalidation_data = sales_train_validation_df[data_colums[-30:]]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:06:12.388247Z","iopub.execute_input":"2022-05-01T02:06:12.388592Z","iopub.status.idle":"2022-05-01T02:06:12.635448Z","shell.execute_reply.started":"2022-05-01T02:06:12.38856Z","shell.execute_reply":"2022-05-01T02:06:12.6344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:05:19.494517Z","iopub.execute_input":"2022-05-01T02:05:19.494798Z","iopub.status.idle":"2022-05-01T02:05:19.501833Z","shell.execute_reply.started":"2022-05-01T02:05:19.494768Z","shell.execute_reply":"2022-05-01T02:05:19.50098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting Training and Validation data","metadata":{}},{"cell_type":"code","source":"\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[0].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=1, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=2, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[100].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[100].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=3, col=1\n)\nfig.update_layout(height=1200, width=800, title_text=\"Training Data[Blue] Validation Data[Orange]\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:06:17.656595Z","iopub.execute_input":"2022-05-01T02:06:17.656913Z","iopub.status.idle":"2022-05-01T02:06:17.729996Z","shell.execute_reply.started":"2022-05-01T02:06:17.656876Z","shell.execute_reply":"2022-05-01T02:06:17.729127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- ![TrainTestSplit](https://i.imgur.com/9jDikEk.png) -->","metadata":{}},{"cell_type":"markdown","source":"## Naive Approach\n\nNaive Approach is very simple approach, we put hypothesis that next day's sales is same as today's sale.\n\n$\\hat{Y_{t+1}} = Y_t$\n\nhere $\\hat{Y_{t+1}}$  is next day's sale and $Y_t$ is today' sale. ","metadata":{}},{"cell_type":"code","source":"predictions = []\nfor index in range(len(validation_data.columns)):\n    if index == 0:\n        # for first validation record prediction will be last day of traindata\n        predictions.append(train_data[train_data.columns[-1]].values)\n    else:\n        predictions.append(validation_data[validation_data.columns[index-1]].values)\n\n# return 30490 * 30 days shape \n# for each store department 30 days prediction in same row\nprint(len(predictions))\npredictions = np.transpose(np.array([row.tolist() for row in predictions]))\n# get error for top 3 store and department record\nerror_naive = np.linalg.norm(predictions- validation_data.values)/ len(predictions[0])\nerror_naive","metadata":{"execution":{"iopub.status.busy":"2022-05-01T03:51:11.207704Z","iopub.execute_input":"2022-05-01T03:51:11.208012Z","iopub.status.idle":"2022-05-01T03:51:11.387793Z","shell.execute_reply.started":"2022-05-01T03:51:11.20798Z","shell.execute_reply":"2022-05-01T03:51:11.386575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[0].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[0], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=1, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=2, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[1], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[100].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[100].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[100], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=3, col=1\n)\nfig.update_layout(height=1200, width=800, title_text=\"Naive Approach [Blue] Validation Data[Orange], Predictions[Green]\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:08:26.799654Z","iopub.execute_input":"2022-05-01T02:08:26.799951Z","iopub.status.idle":"2022-05-01T02:08:26.879777Z","shell.execute_reply.started":"2022-05-01T02:08:26.79992Z","shell.execute_reply":"2022-05-01T02:08:26.878894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- ![Naive Approach](https://i.imgur.com/3Nw4WY7.png) -->","metadata":{}},{"cell_type":"markdown","source":"### Observation:\n\nNaive approch is not useful, it is affected by short term fluctions.","metadata":{}},{"cell_type":"markdown","source":"## Moving Average\n\n* In Moving Average Model we will take the mean unit sales value from last p days, to preidct future values. p is hyper parameter which we need to decide.\n* It takes into account the last p days sales, so not easily affected by short term fluctions if p is reasonably good and large.\n\n<font size=4> $\\hat{y_{t+1}} =\\frac{ \\sum_{t-p}^{t} y_t} {p} $ </font>\n\n* In the above equation $y_{t+1}$ is tomorrow' sale, which can be obtained by summing over last p days' sales and take mean over this. Let's plot graph of Training Data (blue), validation data (orange), and prediction data (green).\n","metadata":{}},{"cell_type":"code","source":"len(train_data.columns)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:17.862138Z","iopub.execute_input":"2022-05-01T02:04:17.862451Z","iopub.status.idle":"2022-05-01T02:04:17.86927Z","shell.execute_reply.started":"2022-05-01T02:04:17.86241Z","shell.execute_reply":"2022-05-01T02:04:17.868363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor i in range(len(validation_data.columns)):\n    if i == 0:\n        predictions.append(np.mean(train_data[train_data.columns[-30:]].values, axis=1))\n    if i < 31 and i > 0:\n        # take mean of \n        train_part =np.mean(train_data[train_data.columns[-30+i:]].values, axis=1)\n        validation_part = np.mean(predictions[:i], axis=0)\n        predictions.append(0.5 * ( train_part + validation_part  ))\n    if i > 31:\n        predictions.append(np.mean([predictions[:i]], axis=1))\n        \nprint(len(predictions))\npredictions = np.transpose(np.array([row.tolist() for row in predictions]))\nerror_avg = np.linalg.norm(predictions - validation_data.values)/len(predictions[0])\nprint(error_avg)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T03:50:38.22965Z","iopub.execute_input":"2022-05-01T03:50:38.229941Z","iopub.status.idle":"2022-05-01T03:50:38.512552Z","shell.execute_reply.started":"2022-05-01T03:50:38.229911Z","shell.execute_reply":"2022-05-01T03:50:38.511497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[0].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[0], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=1, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=2, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[1], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[100].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[100].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[100], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=3, col=1\n)\nfig.update_layout(height=1200, width=800, title_text=\"[Moving Average] Training Data[Blue] Validation Data[Orange], Predictions[Green]\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:18.154198Z","iopub.execute_input":"2022-05-01T02:04:18.154518Z","iopub.status.idle":"2022-05-01T02:04:18.228617Z","shell.execute_reply.started":"2022-05-01T02:04:18.154476Z","shell.execute_reply":"2022-05-01T02:04:18.227766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- ![Moving Average](https://i.imgur.com/v0Dijjq.png) -->","metadata":{}},{"cell_type":"markdown","source":"### Observation:\n\n* Lower error compare to the naive approach, not susceptible to  day-to-day volatility, slightly able to detect trend, but still not accurace to deal with higher trends in sales. ","metadata":{}},{"cell_type":"markdown","source":"# Exponential Smoothing","metadata":{}},{"cell_type":"markdown","source":"In **Exponential smoothing** previous time steps are weighted exponentially and we add **exponentially weighted sum of previous time steps to generate predictions.** \n\n\n<font size=4> $\\hat{y_{t+1}} = \\alpha * y_t + \\alpha* (1-\\alpha) * y_{t-1} + \\alpha * (1-\\alpha)^2 * y_{t-2}    $ \n\n</font>\n\n<font size=4> $\\hat{y_{t+1}} = \\alpha * y_t +  (1-\\alpha) * \\hat{y_{t}}    $ \n\n</font>\n\n\n\n\n\n $\\alpha$   is the smoothing parameter. The forecast $y_{t+1}$ is a weighted average of all the observations in the series y1, … ,yt. The rate at which the weights decay is controlled by the parameter $\\alpha$. \n \n This method is different from simple moving average, because **weight is exponentially decay with time, so past observation have little impact on current prediction**. In simple moving average we weight every past observation equally.\n ","metadata":{}},{"cell_type":"code","source":"train_data.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:18.229946Z","iopub.execute_input":"2022-05-01T02:04:18.230582Z","iopub.status.idle":"2022-05-01T02:04:18.238395Z","shell.execute_reply.started":"2022-05-01T02:04:18.230534Z","shell.execute_reply":"2022-05-01T02:04:18.237197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data.columns[-30:]].values[:3].shape","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:18.239848Z","iopub.execute_input":"2022-05-01T02:04:18.240223Z","iopub.status.idle":"2022-05-01T02:04:18.25532Z","shell.execute_reply.started":"2022-05-01T02:04:18.240176Z","shell.execute_reply":"2022-05-01T02:04:18.254379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:17:49.806629Z","iopub.execute_input":"2022-05-01T02:17:49.806988Z","iopub.status.idle":"2022-05-01T02:17:49.812986Z","shell.execute_reply.started":"2022-05-01T02:17:49.806947Z","shell.execute_reply":"2022-05-01T02:17:49.812256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# predictions = []\n# for row in tqdm(train_data[train_data.columns[-25:]].values):\n#     fit = ExponentialSmoothing(row, seasonal_periods=4).fit()\n#     #https://www.statsmodels.org/dev/examples/notebooks/generated/exponential_smoothing.html\n#     predictions.append(fit.forecast(30))\n# print(len(predictions[0]))\n# predictions = np.array(predictions).reshape((-1, 30))\n\n# print(predictions.shape)\n# print(validation_data.shape)\n# error_exponential = np.linalg.norm(predictions - validation_data.values)/len(predictions[0])\n# error_exponential\n\n\n\npredictions = []\nfor row in tqdm(validation_data[validation_data.columns[-25:]].values):\n    fit = ExponentialSmoothing(row, seasonal_periods=4).fit()\n    #https://www.statsmodels.org/dev/examples/notebooks/generated/exponential_smoothing.html\n    predictions.append(fit.forecast(30))\nprint(len(predictions[0]))\npredictions = np.array(predictions).reshape((-1, 30))\n\nprint(predictions.shape)\nprint(validation_data.shape)\nerror_exponential = np.linalg.norm(predictions - validation_data.values)/len(predictions[0])\nerror_exponential\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:18:05.893087Z","iopub.execute_input":"2022-05-01T02:18:05.893825Z","iopub.status.idle":"2022-05-01T02:21:33.866848Z","shell.execute_reply.started":"2022-05-01T02:18:05.893786Z","shell.execute_reply":"2022-05-01T02:21:33.865802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:15:53.882287Z","iopub.execute_input":"2022-05-01T02:15:53.882727Z","iopub.status.idle":"2022-05-01T02:15:53.897955Z","shell.execute_reply.started":"2022-05-01T02:15:53.882597Z","shell.execute_reply":"2022-05-01T02:15:53.896989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[0].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[0], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=1, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=2, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[1], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[2], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=3, col=1\n)\nfig.update_layout(height=1200, width=800, title_text=\"[Exponential Average] Training Data[Blue] Validation Data[Orange], Predictions[Green]\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:18.346482Z","iopub.execute_input":"2022-05-01T02:04:18.346797Z","iopub.status.idle":"2022-05-01T02:04:18.424736Z","shell.execute_reply.started":"2022-05-01T02:04:18.346756Z","shell.execute_reply":"2022-05-01T02:04:18.423923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- ![Exponential Smoothing](https://i.imgur.com/WVAONGr.png) -->","metadata":{}},{"cell_type":"markdown","source":"* Exponential smoothing generating horizontal line every time,because it gives very low weightage to farwawy timesteps, causing the predictions to flatten out or remain constant.\n","metadata":{}},{"cell_type":"markdown","source":"## ARIMA\n\n","metadata":{}},{"cell_type":"markdown","source":"**ARIMA** stands for **A**uto **R**egressive **I**ntegrated **M**oving **A**verage.\n\nMoving average and. exponential smoothing models were based on a description of trend and seasonality in data, ARIMA models aim to describe the correlations in the time series.","metadata":{}},{"cell_type":"code","source":"predictions = []\n# https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html\n# seasonal_orderiterable, optional\n# The (P,D,Q,s) order of the seasonal component of the model for the AR parameters,\n# differences, MA parameters, and periodicity. \n# D must be an integer indicating the integration order of the process, \n# while P and Q may either be an integers indicating the AR and MA orders \n# (so that all lags up to those orders are included) or else iterables giving specific AR and\n# / or MA lags to include. s is an integer giving the periodicity (number of periods in season), \n# often it is 4 for quarterly data or 12 for monthly data. Default is no seasonal effect.\n# for row in tqdm(train_data[train_data.columns[-25:]].values[:3]):\n#     fit = sm.tsa.statespace.SARIMAX(row, seasonal_order=(0, 1, 1, 7)).fit(disp=0)\n#     # forecasting for 30 days\n    \n#     predictions.append(fit.forecast(30))\n# predictions = np.array(predictions).reshape((-1, 30))\n# ARIMA_error = np.linalg.norm(predictions[:3] - validation_data.values[:3])/len(predictions[0])\n# ARIMA_error\n\nfor row in tqdm(validation_data[validation_data.columns[-25:]].values):\n    fit = sm.tsa.statespace.SARIMAX(row, seasonal_order=(0, 1, 1, 7)).fit(disp=0)\n    # forecasting for 30 days\n    \n    predictions.append(fit.forecast(30))\npredictions = np.array(predictions).reshape((-1, 30))\nARIMA_error = np.linalg.norm(predictions - validation_data.values)/len(predictions[0])\nARIMA_error","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:22:33.384955Z","iopub.execute_input":"2022-05-01T02:22:33.385287Z","iopub.status.idle":"2022-05-01T03:49:01.384445Z","shell.execute_reply.started":"2022-05-01T02:22:33.385256Z","shell.execute_reply":"2022-05-01T03:49:01.382853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[0].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[0], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=1, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=2, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[1], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[2], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=3, col=1\n)\nfig.update_layout(height=1200, width=800, title_text=\"[ARIMA] Training Data[Blue] Validation Data[Orange], Predictions[Green]\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:18.957955Z","iopub.execute_input":"2022-05-01T02:04:18.958553Z","iopub.status.idle":"2022-05-01T02:04:19.079444Z","shell.execute_reply.started":"2022-05-01T02:04:18.958489Z","shell.execute_reply":"2022-05-01T02:04:19.078486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- ![ARIMA](https://i.imgur.com/j5Y5LH8.png) -->","metadata":{}},{"cell_type":"markdown","source":"* ARIMA is able to **find low-level and high-level trends** simultaneously, unlike most other models which can only find one of these. \n\n* **Except the second plot**, ARIMA is able to predict a periodic function for each sample, and these functions seem to be pretty accurate. \n\n* Second plot most of the true value is zero and then very high value, that might be the case if  **stock of that product is not available  during that time**, which **ARIMA failed to capture, ARIMA does not have that information Available .**","metadata":{}},{"cell_type":"markdown","source":"## Prophet\n\n\nProphet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\nYou can Learn more about [prophet](https://facebook.github.io/prophet/)","metadata":{}},{"cell_type":"code","source":"dates = [\"2007-12-\" + str(i) for i in range(1, 31)]\npredictions = []\nfor sales in tqdm(train_data[train_data.columns[-30:]].values[:3]):\n    sales_record_df = pd.DataFrame(np.transpose([dates, sales]))\n    sales_record_df.columns = [\"ds\", \"y\"]\n    prophet_model = Prophet(daily_seasonality=True)\n    prophet_model.fit(sales_record_df)\n    # prediction for next 30 days\n    future = prophet_model.make_future_dataframe(periods=30)\n    forecast = prophet_model.predict(future)[\"yhat\"].loc[30:].values\n    predictions.append(forecast)\n    \npredictions = np.array(predictions).reshape((-1, 30))\nerror_prophet = np.linalg.norm(predictions[:3] - validation_data.values[:3])/len(predictions[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:19.080805Z","iopub.execute_input":"2022-05-01T02:04:19.081519Z","iopub.status.idle":"2022-05-01T02:04:28.227487Z","shell.execute_reply.started":"2022-05-01T02:04:19.081467Z","shell.execute_reply":"2022-05-01T02:04:28.226382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[0].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[0], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=1, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=2, col=1\n)\n\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[1], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_data.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Training Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=validation_data.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Validation Data\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=predictions[2], mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Prediction Data\"),\n    row=3, col=1\n)\nfig.update_layout(height=1200, width=800, title_text=\"[Prophet] Training Data[Blue] Validation Data[Orange], Predictions[Green]\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:28.228814Z","iopub.execute_input":"2022-05-01T02:04:28.229053Z","iopub.status.idle":"2022-05-01T02:04:28.310769Z","shell.execute_reply.started":"2022-05-01T02:04:28.229024Z","shell.execute_reply":"2022-05-01T02:04:28.309783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- ![Prophet](https://i.imgur.com/Rc0Ivu0.png) -->","metadata":{}},{"cell_type":"markdown","source":"### Observation\n\nPerformance of Prophet is very similar to ARIMA.\n","metadata":{}},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"error = [error_naive,error_avg,error_exponential,ARIMA_error, error_prophet]\n\nnames = [\"Naive approach\", \"Moving average\", \"Exponential smoothing\", \"ARIMA\", \"Prophet\"]\n\nmodel_records = {\n                \"Naive approach\":error_naive, \"Moving average\":error_avg,\n                \"Exponential smoothing\":error_exponential, \"ARIMA\":ARIMA_error,\n                \"Prophet\":error_prophet\n                }\nmodel_records = {key:val for key,val in sorted(model_records.items(), key= lambda x: x[1])}\n\ndf= pd.DataFrame.from_dict(model_records,orient='index')\ndf['Model'] =df.index\ndf.columns=[\"Loss\", \"Model\"]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:28.312278Z","iopub.execute_input":"2022-05-01T02:04:28.312551Z","iopub.status.idle":"2022-05-01T02:04:28.401647Z","shell.execute_reply.started":"2022-05-01T02:04:28.312519Z","shell.execute_reply":"2022-05-01T02:04:28.400975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.bar(df, y=\"Loss\", x=\"Model\", color=\"Model\", title=\"Loss vs. Model\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:04:28.40251Z","iopub.execute_input":"2022-05-01T02:04:28.402806Z","iopub.status.idle":"2022-05-01T02:04:29.25722Z","shell.execute_reply.started":"2022-05-01T02:04:28.402777Z","shell.execute_reply":"2022-05-01T02:04:29.256238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- ![Model Results](https://i.imgur.com/aE3DrGZ.png) -->","metadata":{}},{"cell_type":"markdown","source":"### Observation and takeaways\n\n* Exponential Smoothing has least error, Prophet seems to high.\n* When we observe the prophet prediction graph, it clearly capture high and low level trend better than Naive approach, then **Why Naive approach outperformed prophet**?\n* **Answer is that If we carefully look into graph, we have zero unit sales for many days, which might be the case because of zero supply, zero demand, out of stock,** which is not captured by prophet.\n* We need better model which takes into **account about actual demand, supply and stocks, sometimes, holidays, price and lots of features**\n* We need to **spend more time on feature engineering.**","metadata":{}}]}